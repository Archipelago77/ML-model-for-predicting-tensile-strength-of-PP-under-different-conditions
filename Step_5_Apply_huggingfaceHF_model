import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import pandas as pd
import numpy as np
import torch

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

from transformers import (
    DistilBertTokenizerFast,
    DistilBertForSequenceClassification,
    TrainingArguments,
    Trainer
)

# 1. Load and prepare data

# Load Excel file
df = pd.read_excel("507 final project data-2.xlsx")
df.columns = df.columns.str.strip()

# Keep only rows with tensile strength, cycles, and polymer_grade
tensile_df = df.dropna(subset=["tensile_strength_MPa", "recycle_cycles", "polymer_grade"]).copy()

# Build text descriptions for each sample
def row_to_text(row):
    parts = []

    # basic grade + cycles
    parts.append(f"{row['polymer_grade']} with {int(row['recycle_cycles'])} recycling cycles")

    # processing method
    if pd.notna(row.get("processing_method", None)):
        parts.append(f"processed by {row['processing_method']}")

    # glass fiber (most important feature)
    if "wt_glass" in row and pd.notna(row["wt_glass"]) and row["wt_glass"] > 0:
        parts.append(f"containing {row['wt_glass']} percent glass fiber")

    # main contaminants if any
    contaminant_pieces = []
    for name in ["wt_ABS", "wt_PC", "wt_PET", "wt_PLA", "wt_PS", "wt_HDPE"]:
        if name in row and pd.notna(row[name]) and row[name] > 0:
            cont_name = name.replace("wt_", "")
            contaminant_pieces.append(f"{row[name]} percent {cont_name}")

    if contaminant_pieces:
        parts.append("with contaminants: " + ", ".join(contaminant_pieces))

    text = ". ".join(parts)
    return text

tensile_df["text"] = tensile_df.apply(row_to_text, axis=1)
tensile_df["label"] = tensile_df["tensile_strength_MPa"].astype(float)

print("Example text rows:")
print(tensile_df[["text", "label"]].head(), "\n")

# Train-test split
train_df, test_df = train_test_split(
    tensile_df[["text", "label"]],
    test_size=0.3,
    random_state=42
)

# 2. Build a custom PyTorch Dataset

class TextRegressionDataset(torch.utils.data.Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = list(texts)
        self.labels = list(labels)
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = float(self.labels[idx])

        enc = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        # Remove batch dimension from tokenizer output
        item = {k: v.squeeze(0) for k, v in enc.items()}
        item["labels"] = torch.tensor(label, dtype=torch.float)
        return item

# 3. Tokenizer and datasets

model_name = "distilbert-base-uncased"
tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)

train_dataset = TextRegressionDataset(
    texts=train_df["text"],
    labels=train_df["label"],
    tokenizer=tokenizer,
    max_length=128
)

test_dataset = TextRegressionDataset(
    texts=test_df["text"],
    labels=test_df["label"],
    tokenizer=tokenizer,
    max_length=128
)

# 4. Load DistilBERT for regression

model = DistilBertForSequenceClassification.from_pretrained(
    model_name,
    num_labels=1,
    problem_type="regression"
)

# 5. Define metrics and training arguments

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    # predictions shape: (batch_size, 1)
    preds = predictions.reshape(-1)
    labels = labels.reshape(-1)

    mae = mean_absolute_error(labels, preds)
    rmse = np.sqrt(mean_squared_error(labels, preds))
    r2 = r2_score(labels, preds)
    return {
        "mae": mae,
        "rmse": rmse,
        "r2": r2
    }

training_args = TrainingArguments(
    output_dir="./hf_results",
    num_train_epochs=5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=5e-5,
    weight_decay=0.01,
    logging_dir="./hf_logs"
)


# 6. Trainer 

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics
)

# 7. Train and evaluate

trainer.train()

eval_results = trainer.evaluate()
print("\nHF regression results:", eval_results)

